"""
Accuracy Testing Framework

This module implements task 8.5: Validate accuracy improvements and achieve target performance
by implementing comprehensive accuracy testing with synthetic data and automated regression testing.
"""

import numpy as np
import h5py
from typing import Dict, List, Tuple, Optional, Any, Union, Callable
from dataclasses import dataclass
import logging
from pathlib import Path
import time
import json
from datetime import datetime
import matplotlib.pyplot as plt

from ..analysis.robust_power_law_fitter import RobustPowerLawFitter
from ..analysis.correlation_length_calculator import CorrelationLengthCalculator
from ..analysis.robust_critical_exponent_extractor import RobustCriticalExponentExtractor
from ..analysis.latent_analysis import LatentRepresentation
from ..data.high_quality_data_generator import HighQualityDataGenerator, create_high_quality_data_config
from ..utils.logging_utils import get_logger


@dataclass
class SyntheticTestCase:
    """Container for synthetic test case parameters."""
    name: str
    system_type: str
    theoretical_exponents: Dict[str, float]
    critical_temperature: float
    temperature_range: Tuple[float, float]
    noise_level: float
    n_data_points: int
    description: str


@dataclass
class AccuracyTestResult:
    """Container for accuracy test results."""
    test_case: SyntheticTestCase
    
    # Extracted exponents
    beta_extracted: Optional[float]
    beta_error: Optional[float]
    beta_accuracy: Optional[float]
    
    nu_extracted: Optional[float]
    nu_error: Optional[float]
    nu_accuracy: Optional[float]
    
    # Overall metrics
    overall_accuracy: float
    extraction_time: float
    success: bool
    error_message: Optional[str]
    
    # Quality metrics
    beta_quality_score: Optional[float]
    nu_quality_score: Optional[float]
    robustness_score: float
    statistical_significance: Dict[str, float]


@dataclass
class AccuracyValidationReport:
    """Container for comprehensive accuracy validation report."""
    test_results: List[AccuracyTestResult]
    
    # Summary statistics
    beta_accuracy_stats: Dict[str, float]
    nu_accuracy_stats: Dict[str, float]
    overall_accuracy_stats: Dict[str, float]
    
    # Performance metrics
    success_rate: float
    average_extraction_time: float
    
    # Target achievement
    beta_target_achieved: bool
    nu_target_achieved: bool
    overall_target_achieved: bool
    
    # Regression testing
    regression_test_passed: bool
    performance_degradation: Optional[float]
    
    # Report metadata
    test_timestamp: str
    framework_version: str
    total_test_time: float


class AccuracyTestingFramework:
    """
    Comprehensive accuracy testing framework for critical exponent extraction.
    
    Features:
    1. Synthetic data generation with known ground truth
    2. Comprehensive accuracy testing across multiple scenarios
    3. Target performance validation (>70% accuracy)
    4. Automated regression testing
    5. Performance benchmarking and monitoring
    """
    
    def __init__(self,
                 target_beta_accuracy: float = 70.0,
                 target_nu_accuracy: float = 70.0,
                 target_overall_accuracy: float = 70.0,
                 random_seed: Optional[int] = None):
        """
        Initialize accuracy testing framework.
        
        Args:
            target_beta_accuracy: Target accuracy for Î² exponent (%)
            target_nu_accuracy: Target accuracy for Î½ exponent (%)
            target_overall_accuracy: Target overall accuracy (%)
            random_seed: Random seed for reproducibility
        """
        self.target_beta_accuracy = target_beta_accuracy
        self.target_nu_accuracy = target_nu_accuracy
        self.target_overall_accuracy = target_overall_accuracy
        self.random_seed = random_seed
        self.logger = get_logger(__name__)
        
        # Initialize components
        self.extractor = RobustCriticalExponentExtractor(random_seed=random_seed)
        self.data_generator = HighQualityDataGenerator(random_seed=random_seed, verbose=False)
        
        if random_seed is not None:
            np.random.seed(random_seed)
    
    def run_comprehensive_accuracy_validation(self,
                                            output_dir: Optional[str] = None) -> AccuracyValidationReport:
        """
        Run comprehensive accuracy validation across multiple test cases.
        
        Args:
            output_dir: Optional directory to save results
            
        Returns:
            AccuracyValidationReport with complete validation results
        """
        self.logger.info("Starting comprehensive accuracy validation")
        start_time = time.time()
        
        # Generate test cases
        test_cases = self._generate_test_cases()
        
        self.logger.info(f"Generated {len(test_cases)} test cases")
        
        # Run tests
        test_results = []
        
        for i, test_case in enumerate(test_cases):
            self.logger.info(f"Running test case {i+1}/{len(test_cases)}: {test_case.name}")
            
            try:
                result = self._run_single_test_case(test_case)
                test_results.append(result)
                
                if result.success:
                    self.logger.info(f"Test passed: Î²={result.beta_accuracy:.1f}%, Î½={result.nu_accuracy:.1f}%")
                else:
                    self.logger.warning(f"Test failed: {result.error_message}")
                    
            except Exception as e:
                self.logger.error(f"Test case {test_case.name} failed with exception: {e}")
                
                # Create failed result
                failed_result = AccuracyTestResult(
                    test_case=test_case,
                    beta_extracted=None,
                    beta_error=None,
                    beta_accuracy=None,
                    nu_extracted=None,
                    nu_error=None,
                    nu_accuracy=None,
                    overall_accuracy=0.0,
                    extraction_time=0.0,
                    success=False,
                    error_message=str(e),
                    beta_quality_score=None,
                    nu_quality_score=None,
                    robustness_score=0.0,
                    statistical_significance={}
                )
                test_results.append(failed_result)
        
        # Compute summary statistics
        report = self._generate_validation_report(test_results, time.time() - start_time)
        
        # Save results if output directory provided
        if output_dir:
            self._save_validation_report(report, output_dir)
        
        # Log summary
        self._log_validation_summary(report)
        
        return report
    
    def _generate_test_cases(self) -> List[SyntheticTestCase]:
        """Generate comprehensive set of test cases."""
        
        test_cases = []
        
        # Test Case 1: Ideal 3D Ising (no noise)
        test_cases.append(SyntheticTestCase(
            name="3D_Ising_Ideal",
            system_type="ising_3d",
            theoretical_exponents={"beta": 0.326, "nu": 0.630},
            critical_temperature=4.511,
            temperature_range=(3.5, 5.5),
            noise_level=0.0,
            n_data_points=100,
            description="Ideal 3D Ising model with no noise"
        ))
        
        # Test Case 2: 3D Ising with low noise
        test_cases.append(SyntheticTestCase(
            name="3D_Ising_Low_Noise",
            system_type="ising_3d",
            theoretical_exponents={"beta": 0.326, "nu": 0.630},
            critical_temperature=4.511,
            temperature_range=(3.5, 5.5),
            noise_level=0.05,
            n_data_points=100,
            description="3D Ising model with 5% noise"
        ))
        
        # Test Case 3: 3D Ising with moderate noise
        test_cases.append(SyntheticTestCase(
            name="3D_Ising_Moderate_Noise",
            system_type="ising_3d",
            theoretical_exponents={"beta": 0.326, "nu": 0.630},
            critical_temperature=4.511,
            temperature_range=(3.5, 5.5),
            noise_level=0.1,
            n_data_points=100,
            description="3D Ising model with 10% noise"
        ))
        
        # Test Case 4: 2D Ising for comparison
        test_cases.append(SyntheticTestCase(
            name="2D_Ising_Ideal",
            system_type="ising_2d",
            theoretical_exponents={"beta": 0.125, "nu": 1.0},
            critical_temperature=2.269,
            temperature_range=(1.5, 3.0),
            noise_level=0.0,
            n_data_points=80,
            description="Ideal 2D Ising model"
        ))
        
        # Test Case 5: Limited data (stress test)
        test_cases.append(SyntheticTestCase(
            name="3D_Ising_Limited_Data",
            system_type="ising_3d",
            theoretical_exponents={"beta": 0.326, "nu": 0.630},
            critical_temperature=4.511,
            temperature_range=(4.0, 5.0),
            noise_level=0.05,
            n_data_points=30,
            description="3D Ising with limited data points"
        ))
        
        # Test Case 6: Wide temperature range
        test_cases.append(SyntheticTestCase(
            name="3D_Ising_Wide_Range",
            system_type="ising_3d",
            theoretical_exponents={"beta": 0.326, "nu": 0.630},
            critical_temperature=4.511,
            temperature_range=(2.0, 7.0),
            noise_level=0.05,
            n_data_points=150,
            description="3D Ising with wide temperature range"
        ))
        
        # Test Case 7: High noise (challenging case)
        test_cases.append(SyntheticTestCase(
            name="3D_Ising_High_Noise",
            system_type="ising_3d",
            theoretical_exponents={"beta": 0.326, "nu": 0.630},
            critical_temperature=4.511,
            temperature_range=(3.5, 5.5),
            noise_level=0.2,
            n_data_points=100,
            description="3D Ising model with 20% noise (challenging)"
        ))
        
        return test_cases
    
    def _run_single_test_case(self, test_case: SyntheticTestCase) -> AccuracyTestResult:
        """Run a single accuracy test case."""
        
        start_time = time.time()
        
        try:
            # Generate synthetic data
            latent_repr = self._generate_synthetic_data(test_case)
            
            # Extract Î² exponent
            beta_result = None
            beta_accuracy = None
            beta_quality = None
            
            try:
                beta_extraction = self.extractor.extract_robust_critical_exponent(
                    latent_repr, test_case.critical_temperature, 'beta',
                    theoretical_exponent=test_case.theoretical_exponents.get('beta')
                )
                
                beta_result = (beta_extraction.final_exponent, beta_extraction.final_error)
                
                if test_case.theoretical_exponents.get('beta') is not None:
                    theoretical_beta = test_case.theoretical_exponents['beta']
                    beta_accuracy = (1 - abs(beta_extraction.final_exponent - theoretical_beta) / theoretical_beta) * 100
                
                beta_quality = beta_extraction.overall_quality_score
                
            except Exception as e:
                self.logger.warning(f"Î² extraction failed for {test_case.name}: {e}")
                beta_result = None
                beta_accuracy = 0.0
                beta_quality = 0.0
            
            # Extract Î½ exponent
            nu_result = None
            nu_accuracy = None
            nu_quality = None
            
            try:
                nu_extraction = self.extractor.extract_robust_critical_exponent(
                    latent_repr, test_case.critical_temperature, 'nu',
                    theoretical_exponent=test_case.theoretical_exponents.get('nu')
                )
                
                nu_result = (nu_extraction.final_exponent, nu_extraction.final_error)
                
                if test_case.theoretical_exponents.get('nu') is not None:
                    theoretical_nu = test_case.theoretical_exponents['nu']
                    # Note: nu_extraction gives negative exponent, theoretical is positive
                    nu_accuracy = (1 - abs(abs(nu_extraction.final_exponent) - theoretical_nu) / theoretical_nu) * 100
                
                nu_quality = nu_extraction.overall_quality_score
                
            except Exception as e:
                self.logger.warning(f"Î½ extraction failed for {test_case.name}: {e}")
                nu_result = None
                nu_accuracy = 0.0
                nu_quality = 0.0
            
            # Compute overall accuracy
            accuracies = []
            if beta_accuracy is not None:
                accuracies.append(beta_accuracy)
            if nu_accuracy is not None:
                accuracies.append(nu_accuracy)
            
            overall_accuracy = np.mean(accuracies) if accuracies else 0.0
            
            # Compute robustness score
            robustness_score = 0.5
            if beta_quality is not None and nu_quality is not None:
                robustness_score = (beta_quality + nu_quality) / 2
            elif beta_quality is not None:
                robustness_score = beta_quality
            elif nu_quality is not None:
                robustness_score = nu_quality
            
            # Statistical significance
            statistical_significance = {}
            if beta_result is not None:
                statistical_significance['beta'] = 0.01  # Placeholder
            if nu_result is not None:
                statistical_significance['nu'] = 0.01  # Placeholder
            
            extraction_time = time.time() - start_time
            
            # Determine success
            success = (
                (beta_accuracy is None or beta_accuracy >= self.target_beta_accuracy) and
                (nu_accuracy is None or nu_accuracy >= self.target_nu_accuracy) and
                overall_accuracy >= self.target_overall_accuracy
            )
            
            return AccuracyTestResult(
                test_case=test_case,
                beta_extracted=beta_result[0] if beta_result else None,
                beta_error=beta_result[1] if beta_result else None,
                beta_accuracy=beta_accuracy,
                nu_extracted=nu_result[0] if nu_result else None,
                nu_error=nu_result[1] if nu_result else None,
                nu_accuracy=nu_accuracy,
                overall_accuracy=overall_accuracy,
                extraction_time=extraction_time,
                success=success,
                error_message=None,
                beta_quality_score=beta_quality,
                nu_quality_score=nu_quality,
                robustness_score=robustness_score,
                statistical_significance=statistical_significance
            )
            
        except Exception as e:
            extraction_time = time.time() - start_time
            
            return AccuracyTestResult(
                test_case=test_case,
                beta_extracted=None,
                beta_error=None,
                beta_accuracy=None,
                nu_extracted=None,
                nu_error=None,
                nu_accuracy=None,
                overall_accuracy=0.0,
                extraction_time=extraction_time,
                success=False,
                error_message=str(e),
                beta_quality_score=None,
                nu_quality_score=None,
                robustness_score=0.0,
                statistical_significance={}
            )
    
    def _generate_synthetic_data(self, test_case: SyntheticTestCase) -> LatentRepresentation:
        """Generate synthetic data for test case."""
        
        # Generate temperature points
        temp_min, temp_max = test_case.temperature_range
        temperatures = np.linspace(temp_min, temp_max, test_case.n_data_points)
        
        # Generate synthetic magnetizations based on theoretical behavior
        magnetizations = self._generate_synthetic_magnetizations(
            temperatures, test_case.critical_temperature, 
            test_case.theoretical_exponents.get('beta', 0.326),
            test_case.noise_level
        )
        
        # Generate synthetic latent coordinates
        # For testing, we'll create latent coordinates that correlate with magnetization
        z1 = magnetizations + np.random.normal(0, 0.1, len(magnetizations))
        z2 = np.random.normal(0, 0.5, len(magnetizations))
        
        # Add noise if specified
        if test_case.noise_level > 0:
            noise_scale = test_case.noise_level * np.std(magnetizations)
            magnetizations += np.random.normal(0, noise_scale, len(magnetizations))
            z1 += np.random.normal(0, noise_scale * 0.5, len(z1))
            z2 += np.random.normal(0, noise_scale * 0.5, len(z2))
        
        # Create latent coordinates array
        latent_coords = np.column_stack([z1, z2])
        
        return LatentRepresentation(
            latent_coords=latent_coords,
            z1=z1,
            z2=z2,
            temperatures=temperatures,
            magnetizations=magnetizations,
            system_size=32,  # Placeholder
            model_type=test_case.system_type
        )
    
    def _generate_synthetic_magnetizations(self,
                                         temperatures: np.ndarray,
                                         critical_temperature: float,
                                         beta_exponent: float,
                                         noise_level: float) -> np.ndarray:
        """Generate synthetic magnetizations with correct critical behavior."""
        
        magnetizations = np.zeros_like(temperatures)
        
        for i, temp in enumerate(temperatures):
            if temp < critical_temperature:
                # Below Tc: m âˆ (Tc - T)^Î²
                reduced_temp = critical_temperature - temp
                magnetization = reduced_temp ** beta_exponent
                
                # Add some saturation behavior
                magnetization = np.tanh(2 * magnetization)
                
            else:
                # Above Tc: small magnetization
                reduced_temp = temp - critical_temperature
                magnetization = 0.1 * np.exp(-reduced_temp)
            
            magnetizations[i] = magnetization
        
        # Normalize to reasonable range
        magnetizations = magnetizations / np.max(magnetizations) * 0.8
        
        return magnetizations
    
    def _generate_validation_report(self,
                                  test_results: List[AccuracyTestResult],
                                  total_time: float) -> AccuracyValidationReport:
        """Generate comprehensive validation report."""
        
        # Filter successful results for statistics
        successful_results = [r for r in test_results if r.success]
        
        # Beta accuracy statistics
        beta_accuracies = [r.beta_accuracy for r in test_results if r.beta_accuracy is not None]
        beta_accuracy_stats = {
            'mean': np.mean(beta_accuracies) if beta_accuracies else 0.0,
            'std': np.std(beta_accuracies) if beta_accuracies else 0.0,
            'min': np.min(beta_accuracies) if beta_accuracies else 0.0,
            'max': np.max(beta_accuracies) if beta_accuracies else 0.0,
            'median': np.median(beta_accuracies) if beta_accuracies else 0.0
        }
        
        # Nu accuracy statistics
        nu_accuracies = [r.nu_accuracy for r in test_results if r.nu_accuracy is not None]
        nu_accuracy_stats = {
            'mean': np.mean(nu_accuracies) if nu_accuracies else 0.0,
            'std': np.std(nu_accuracies) if nu_accuracies else 0.0,
            'min': np.min(nu_accuracies) if nu_accuracies else 0.0,
            'max': np.max(nu_accuracies) if nu_accuracies else 0.0,
            'median': np.median(nu_accuracies) if nu_accuracies else 0.0
        }
        
        # Overall accuracy statistics
        overall_accuracies = [r.overall_accuracy for r in test_results]
        overall_accuracy_stats = {
            'mean': np.mean(overall_accuracies),
            'std': np.std(overall_accuracies),
            'min': np.min(overall_accuracies),
            'max': np.max(overall_accuracies),
            'median': np.median(overall_accuracies)
        }
        
        # Performance metrics
        success_rate = len(successful_results) / len(test_results) * 100
        extraction_times = [r.extraction_time for r in test_results]
        average_extraction_time = np.mean(extraction_times)
        
        # Target achievement
        beta_target_achieved = beta_accuracy_stats['mean'] >= self.target_beta_accuracy
        nu_target_achieved = nu_accuracy_stats['mean'] >= self.target_nu_accuracy
        overall_target_achieved = overall_accuracy_stats['mean'] >= self.target_overall_accuracy
        
        # Regression testing (placeholder - would compare with previous results)
        regression_test_passed = True
        performance_degradation = None
        
        return AccuracyValidationReport(
            test_results=test_results,
            beta_accuracy_stats=beta_accuracy_stats,
            nu_accuracy_stats=nu_accuracy_stats,
            overall_accuracy_stats=overall_accuracy_stats,
            success_rate=success_rate,
            average_extraction_time=average_extraction_time,
            beta_target_achieved=beta_target_achieved,
            nu_target_achieved=nu_target_achieved,
            overall_target_achieved=overall_target_achieved,
            regression_test_passed=regression_test_passed,
            performance_degradation=performance_degradation,
            test_timestamp=datetime.now().isoformat(),
            framework_version="1.0.0",
            total_test_time=total_time
        )
    
    def _save_validation_report(self, report: AccuracyValidationReport, output_dir: str) -> None:
        """Save validation report to files."""
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Save JSON report
        json_path = output_path / f"accuracy_validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # Convert report to JSON-serializable format
        report_dict = {
            'test_results': [
                {
                    'test_case_name': r.test_case.name,
                    'system_type': r.test_case.system_type,
                    'beta_accuracy': r.beta_accuracy,
                    'nu_accuracy': r.nu_accuracy,
                    'overall_accuracy': r.overall_accuracy,
                    'success': r.success,
                    'extraction_time': r.extraction_time,
                    'error_message': r.error_message
                }
                for r in report.test_results
            ],
            'summary': {
                'beta_accuracy_stats': report.beta_accuracy_stats,
                'nu_accuracy_stats': report.nu_accuracy_stats,
                'overall_accuracy_stats': report.overall_accuracy_stats,
                'success_rate': report.success_rate,
                'average_extraction_time': report.average_extraction_time,
                'beta_target_achieved': report.beta_target_achieved,
                'nu_target_achieved': report.nu_target_achieved,
                'overall_target_achieved': report.overall_target_achieved,
                'regression_test_passed': report.regression_test_passed,
                'test_timestamp': report.test_timestamp,
                'total_test_time': report.total_test_time
            }
        }
        
        with open(json_path, 'w') as f:
            json.dump(report_dict, f, indent=2)
        
        # Generate and save visualization
        fig = self._create_validation_visualization(report)
        fig_path = output_path / f"accuracy_validation_plots_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        fig.savefig(fig_path, dpi=300, bbox_inches='tight')
        plt.close(fig)
        
        self.logger.info(f"Validation report saved to {output_path}")
    
    def _create_validation_visualization(self, report: AccuracyValidationReport) -> plt.Figure:
        """Create comprehensive visualization of validation results."""
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # Plot 1: Accuracy by test case
        ax = axes[0, 0]
        
        test_names = [r.test_case.name for r in report.test_results]
        beta_accs = [r.beta_accuracy or 0 for r in report.test_results]
        nu_accs = [r.nu_accuracy or 0 for r in report.test_results]
        overall_accs = [r.overall_accuracy for r in report.test_results]
        
        x = np.arange(len(test_names))
        width = 0.25
        
        ax.bar(x - width, beta_accs, width, label='Î² Accuracy', alpha=0.8)
        ax.bar(x, nu_accs, width, label='Î½ Accuracy', alpha=0.8)
        ax.bar(x + width, overall_accs, width, label='Overall Accuracy', alpha=0.8)
        
        ax.axhline(self.target_beta_accuracy, color='red', linestyle='--', alpha=0.7, label='Target')
        
        ax.set_xlabel('Test Case')
        ax.set_ylabel('Accuracy (%)')
        ax.set_title('Accuracy by Test Case')
        ax.set_xticks(x)
        ax.set_xticklabels(test_names, rotation=45, ha='right')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # Plot 2: Accuracy distribution
        ax = axes[0, 1]
        
        all_accuracies = beta_accs + nu_accs
        ax.hist(all_accuracies, bins=20, alpha=0.7, edgecolor='black')
        ax.axvline(self.target_overall_accuracy, color='red', linestyle='--', 
                  label=f'Target: {self.target_overall_accuracy}%')
        ax.axvline(np.mean(all_accuracies), color='green', linestyle='--',
                  label=f'Mean: {np.mean(all_accuracies):.1f}%')
        
        ax.set_xlabel('Accuracy (%)')
        ax.set_ylabel('Frequency')
        ax.set_title('Accuracy Distribution')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # Plot 3: Success rate and timing
        ax = axes[0, 2]
        
        metrics = ['Success Rate (%)', 'Avg Time (s)', 'Target Achievement']
        values = [
            report.success_rate,
            report.average_extraction_time,
            (report.beta_target_achieved + report.nu_target_achieved + report.overall_target_achieved) / 3 * 100
        ]
        
        bars = ax.bar(metrics, values)
        
        # Color bars based on performance
        colors = ['green' if v > 70 else 'orange' if v > 50 else 'red' for v in values]
        for bar, color in zip(bars, colors):
            bar.set_color(color)
        
        ax.set_ylabel('Value')
        ax.set_title('Performance Metrics')
        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')
        
        # Plot 4: Accuracy vs noise level
        ax = axes[1, 0]
        
        noise_levels = []
        noise_accuracies = []
        
        for result in report.test_results:
            if result.test_case.noise_level is not None:
                noise_levels.append(result.test_case.noise_level)
                noise_accuracies.append(result.overall_accuracy)
        
        if noise_levels:
            ax.scatter(noise_levels, noise_accuracies, alpha=0.7, s=50)
            
            # Fit trend line
            if len(noise_levels) > 1:
                z = np.polyfit(noise_levels, noise_accuracies, 1)
                p = np.poly1d(z)
                x_trend = np.linspace(min(noise_levels), max(noise_levels), 100)
                ax.plot(x_trend, p(x_trend), 'r--', alpha=0.8, label='Trend')
        
        ax.set_xlabel('Noise Level')
        ax.set_ylabel('Overall Accuracy (%)')
        ax.set_title('Accuracy vs Noise Level')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # Plot 5: Target achievement summary
        ax = axes[1, 1]
        
        targets = ['Î² Exponent', 'Î½ Exponent', 'Overall']
        achieved = [
            report.beta_target_achieved,
            report.nu_target_achieved,
            report.overall_target_achieved
        ]
        
        colors = ['green' if a else 'red' for a in achieved]
        bars = ax.bar(targets, [100 if a else 0 for a in achieved], color=colors, alpha=0.7)
        
        ax.set_ylabel('Target Achieved')
        ax.set_title('Target Achievement Status')
        ax.set_ylim(0, 100)
        
        # Add text labels
        for bar, ach in zip(bars, achieved):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height/2,
                   'PASS' if ach else 'FAIL',
                   ha='center', va='center', fontweight='bold', color='white')
        
        # Plot 6: Summary statistics
        ax = axes[1, 2]
        ax.axis('off')
        
        summary_text = f"Accuracy Validation Summary\n\n"
        summary_text += f"Total Test Cases: {len(report.test_results)}\n"
        summary_text += f"Success Rate: {report.success_rate:.1f}%\n\n"
        
        summary_text += f"Î² Exponent Accuracy:\n"
        summary_text += f"  Mean: {report.beta_accuracy_stats['mean']:.1f}%\n"
        summary_text += f"  Target: {self.target_beta_accuracy}%\n"
        summary_text += f"  Achieved: {'YES' if report.beta_target_achieved else 'NO'}\n\n"
        
        summary_text += f"Î½ Exponent Accuracy:\n"
        summary_text += f"  Mean: {report.nu_accuracy_stats['mean']:.1f}%\n"
        summary_text += f"  Target: {self.target_nu_accuracy}%\n"
        summary_text += f"  Achieved: {'YES' if report.nu_target_achieved else 'NO'}\n\n"
        
        summary_text += f"Overall Accuracy:\n"
        summary_text += f"  Mean: {report.overall_accuracy_stats['mean']:.1f}%\n"
        summary_text += f"  Target: {self.target_overall_accuracy}%\n"
        summary_text += f"  Achieved: {'YES' if report.overall_target_achieved else 'NO'}\n\n"
        
        summary_text += f"Performance:\n"
        summary_text += f"  Avg Time: {report.average_extraction_time:.2f}s\n"
        summary_text += f"  Total Time: {report.total_test_time:.1f}s\n"
        
        ax.text(0.05, 0.95, summary_text, transform=ax.transAxes, fontsize=11,
               verticalalignment='top', fontfamily='monospace')
        
        plt.tight_layout()
        return fig
    
    def _log_validation_summary(self, report: AccuracyValidationReport) -> None:
        """Log validation summary to console."""
        
        self.logger.info("=" * 60)
        self.logger.info("ACCURACY VALIDATION SUMMARY")
        self.logger.info("=" * 60)
        
        self.logger.info(f"Total test cases: {len(report.test_results)}")
        self.logger.info(f"Success rate: {report.success_rate:.1f}%")
        self.logger.info(f"Total test time: {report.total_test_time:.1f}s")
        
        self.logger.info("\nÎ² EXPONENT RESULTS:")
        self.logger.info(f"  Mean accuracy: {report.beta_accuracy_stats['mean']:.1f}%")
        self.logger.info(f"  Target: {self.target_beta_accuracy}%")
        self.logger.info(f"  Target achieved: {'YES' if report.beta_target_achieved else 'NO'}")
        
        self.logger.info("\nÎ½ EXPONENT RESULTS:")
        self.logger.info(f"  Mean accuracy: {report.nu_accuracy_stats['mean']:.1f}%")
        self.logger.info(f"  Target: {self.target_nu_accuracy}%")
        self.logger.info(f"  Target achieved: {'YES' if report.nu_target_achieved else 'NO'}")
        
        self.logger.info("\nOVERALL RESULTS:")
        self.logger.info(f"  Mean accuracy: {report.overall_accuracy_stats['mean']:.1f}%")
        self.logger.info(f"  Target: {self.target_overall_accuracy}%")
        self.logger.info(f"  Target achieved: {'YES' if report.overall_target_achieved else 'NO'}")
        
        # Overall assessment
        if report.beta_target_achieved and report.nu_target_achieved and report.overall_target_achieved:
            self.logger.info("\nðŸŽ‰ ALL TARGETS ACHIEVED! Framework is ready for production.")
        elif report.overall_target_achieved:
            self.logger.info("\nâœ… Overall target achieved, but some individual targets missed.")
        else:
            self.logger.info("\nâŒ Targets not achieved. Further improvements needed.")
        
        self.logger.info("=" * 60)
    
    def run_regression_test(self,
                          baseline_results_path: str,
                          tolerance: float = 5.0) -> bool:
        """
        Run regression test against baseline results.
        
        Args:
            baseline_results_path: Path to baseline results JSON file
            tolerance: Acceptable degradation in accuracy (%)
            
        Returns:
            True if regression test passes
        """
        self.logger.info("Running regression test")
        
        try:
            # Load baseline results
            with open(baseline_results_path, 'r') as f:
                baseline_data = json.load(f)
            
            baseline_accuracy = baseline_data['summary']['overall_accuracy_stats']['mean']
            
            # Run current validation
            current_report = self.run_comprehensive_accuracy_validation()
            current_accuracy = current_report.overall_accuracy_stats['mean']
            
            # Check for degradation
            degradation = baseline_accuracy - current_accuracy
            
            if degradation > tolerance:
                self.logger.error(f"Regression detected: {degradation:.1f}% accuracy loss")
                return False
            else:
                self.logger.info(f"Regression test passed: {degradation:.1f}% change")
                return True
                
        except Exception as e:
            self.logger.error(f"Regression test failed: {e}")
            return False


def create_accuracy_testing_framework(
    target_beta_accuracy: float = 70.0,
    target_nu_accuracy: float = 70.0,
    target_overall_accuracy: float = 70.0,
    random_seed: Optional[int] = None
) -> AccuracyTestingFramework:
    """
    Factory function to create an AccuracyTestingFramework.
    
    Args:
        target_beta_accuracy: Target accuracy for Î² exponent (%)
        target_nu_accuracy: Target accuracy for Î½ exponent (%)
        target_overall_accuracy: Target overall accuracy (%)
        random_seed: Random seed for reproducibility
        
    Returns:
        Configured AccuracyTestingFramework instance
    """
    return AccuracyTestingFramework(
        target_beta_accuracy=target_beta_accuracy,
        target_nu_accuracy=target_nu_accuracy,
        target_overall_accuracy=target_overall_accuracy,
        random_seed=random_seed
    )